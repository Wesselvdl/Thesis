{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8510ff0a-4ecf-4a8a-8ca7-9fea06d06aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bert_score import score as bert_score\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89dbf7bd-ace5-4456-bddc-b19547e0d299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Used Accuracy: 96.00%\n",
      "Cypher Query Accuracy: 64.00%\n",
      "Average Time Taken: 8.31 seconds\n",
      "Variance in Time Taken: 16.36\n",
      "Standard Deviation in Time Taken: 4.04\n"
     ]
    }
   ],
   "source": [
    "# Define correct answers\n",
    "correct_tool = \"Cypher\"\n",
    "correct_cypher_query = \"MATCH (reg:Regulation) WHERE reg.ID IN ['3.2.8', '4.2.1', '5.1.1'] RETURN reg.ID AS RegulationID, reg.Description AS RegulationDescription, reg.Value AS RegulationValue, reg.Notes AS RegulationNotes, reg.`Referred Documentation` AS RegulationDocumentation\"\n",
    "\n",
    "# Function to normalize text for comparison\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return ' '.join(str(text).strip().split()).lower()\n",
    "\n",
    "# Analyze the \"Tool Used\" column\n",
    "tool_accuracy = results_df['Tool Used'].value_counts(normalize=True).get(correct_tool, 0)\n",
    "\n",
    "# Analyze the \"Cypher Query\" column with normalized comparison, only if the correct tool was selected\n",
    "cypher_accuracy = results_df.apply(lambda row: normalize_text(row['Cypher Query']) == normalize_text(correct_cypher_query) if row['Tool Used'] == correct_tool else False, axis=1).mean()\n",
    "\n",
    "# Analyze the \"Time The Prompt Took\" column\n",
    "average_time = results_df['Time The Prompt Took'].mean()\n",
    "time_variance = results_df['Time The Prompt Took'].var()\n",
    "time_std_dev = results_df['Time The Prompt Took'].std()\n",
    "\n",
    "print(f\"Tool Used Accuracy: {tool_accuracy * 100:.2f}%\")\n",
    "print(f\"Cypher Query Accuracy: {cypher_accuracy * 100:.2f}%\")\n",
    "print(f\"Average Time Taken: {average_time:.2f} seconds\")\n",
    "print(f\"Variance in Time Taken: {time_variance:.2f}\")\n",
    "print(f\"Standard Deviation in Time Taken: {time_std_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03101b37-d11a-4dde-b3d3-4f6657288ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59477a3772984946b2a7bb42f39ac91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4a11b764074003a5f7ad442521ec8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 7.91 seconds, 12.64 sentences/sec\n",
      "Mean BERTScore F1: 0.9100011587142944\n",
      "Variance of BERTScore F1: 0.003703262796907575\n",
      "Standard Deviation of BERTScore F1: 0.06085443941823451\n",
      "Mean ROUGE-1 F1: 0.5407024092395128\n",
      "Mean ROUGE-2 F1: 0.3396871363256173\n",
      "Mean ROUGE-L F1: 0.5084143227730317\n"
     ]
    }
   ],
   "source": [
    "# Load your results CSV\n",
    "file_path = 'Results.csv'\n",
    "results_df = pd.read_csv(file_path)\n",
    "\n",
    "# Expert-provided correct action plan\n",
    "expert_action_plan = \"Action Plan: 3.2.8: Evaluate the percentage of energy consumed by the data center that is derived from renewable or sustainable sources. Document and report this percentage in comparison to the total energy consumption. Use standardized metrics such as EN 50600-4-3 or ISO/IEC 30134-3 for accurate measurement. - 4  Notes: Standardized metrics available in EN 50600-4-3 or ISO/IEC 30134-3. Referred Documentation: EN 50600-4-3, ISO/IEC 30134-3, CLC 50600-5-1. 4.2.1: Establish processes that require approval from senior management for any new service that necessitates dedicated hardware, including servers, storage, and networking components, which do not operate on a resource-sharing platform. - 4  5.1.1: Ensure that IT equipment in cabinets shares the same airflow direction. Implement the hot/cold aisle arrangement to align airflow in and across cabinets and aisles, enhancing the design with fully blanked empty cabinets or solid doors. - 4\"\n",
    "\n",
    "# Prepare data for BERTScore and ROUGE\n",
    "references = [expert_action_plan] * len(results_df)\n",
    "hypotheses = results_df['Final Answer'].tolist()\n",
    "\n",
    "# Calculate BERT Scores\n",
    "P, R, F1 = bert_score(hypotheses, references, lang=\"en\", verbose=True)\n",
    "mean_bert_f1 = F1.mean().item()\n",
    "variance_bert_f1 = np.var(F1.tolist())\n",
    "std_dev_bert_f1 = np.std(F1.tolist())\n",
    "\n",
    "print(f\"Mean BERTScore F1: {mean_bert_f1}\")\n",
    "print(f\"Variance of BERTScore F1: {variance_bert_f1}\")\n",
    "print(f\"Standard Deviation of BERTScore F1: {std_dev_bert_f1}\")\n",
    "\n",
    "# Calculate ROUGE Scores\n",
    "rouge = Rouge()\n",
    "rouge_scores = rouge.get_scores(hypotheses, references, avg=True)\n",
    "\n",
    "mean_rouge_1 = rouge_scores['rouge-1']['f']\n",
    "mean_rouge_2 = rouge_scores['rouge-2']['f']\n",
    "mean_rouge_l = rouge_scores['rouge-l']['f']\n",
    "\n",
    "print(f\"Mean ROUGE-1 F1: {mean_rouge_1}\")\n",
    "print(f\"Mean ROUGE-2 F1: {mean_rouge_2}\")\n",
    "print(f\"Mean ROUGE-L F1: {mean_rouge_l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a23f62e-aebd-4a1c-9d59-647a3cf1db64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc32c9a0-6c58-41e4-921f-add032d6d52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
